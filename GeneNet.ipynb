{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "if socket.gethostname() == 'dlm':\n",
    "  %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "  %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "import requests, zipfile, io\n",
    "import pickle\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "import networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lib_path = 'I:/code'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/media/6T/.tianle/.lib'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/projects/academic/azhang/tianlema/lib'\n",
    "if os.path.exists(lib_path) and lib_path not in sys.path:\n",
    "  sys.path.append(lib_path)\n",
    "\n",
    "from dl.models.transformer import MultiheadAttention, EncoderAttention, DecoderAttention, Transformer, StackedEncoder\n",
    "from dl.models.dag import *\n",
    "from dl.models.basic_models import *\n",
    "from dl.models.factor_graph import BipartiteGraph1d, BipartiteGraph, EmbedCell, GeneNet, PathNet, GraphConvolution1d\n",
    "from dl.utils.visualization.visualization import *\n",
    "from dl.utils.outlier import *\n",
    "from dl.utils.train import *\n",
    "from autoencoder.autoencoder import *\n",
    "from vin.vin import *\n",
    "from dl.utils.utils import get_overlap_samples, filter_clinical_dict, get_target_variable\n",
    "from dl.utils.utils import get_shuffled_data, target_to_numpy, discrete_to_id, get_mi_acc\n",
    "from dl.utils.utils import get_label_distribution, normalize_continuous_variable, tensor_to_num\n",
    "from dl.utils.utils import adj_list_to_mat, adj_list_to_attention_mats, count_model_parameters\n",
    "from dl.utils.utils import get_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "  print('Using GPU:)')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Using CPU:(')\n",
    "  \n",
    "inf = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jia_data = False\n",
    "sel_proj_id = 'BRCA'\n",
    "target_name = 'tumor_stage'\n",
    "split_portion = [1, 1, 8]\n",
    "seed = 0\n",
    "randomize_labels = False\n",
    "init_num_gene = 5000\n",
    "min_num_gene_per_go = 5\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 50\n",
    "reduce_every = 100\n",
    "batch_size = None\n",
    "print_every = 1\n",
    "eval_every = 1\n",
    "return_best_val = True\n",
    "result_folder = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_portion_str = np.array(split_portion)\n",
    "split_portion_str = split_portion_str * 100 / split_portion_str.sum()\n",
    "split_portion_str = '-'.join(map(lambda s: str(int(s)), split_portion_str))\n",
    "res_filename_prefix = f'{sel_proj_id}_{split_portion_str}_seed{seed}_{target_name}'\n",
    "res_filename = f'{res_filename_prefix}.pkl'\n",
    "if not os.path.exists(result_folder):\n",
    "  os.makedirs(result_folder)\n",
    "if use_jia_data:\n",
    "  sel_proj_id = 'JIA'\n",
    "else:\n",
    "  if isinstance(sel_proj_id, str):\n",
    "    if not sel_proj_id.startswith('TCGA-'):\n",
    "      sel_proj_id = 'TCGA-' + sel_proj_id\n",
    "      sel_proj_list = [sel_proj_id]\n",
    "  elif isinstance(sel_proj_id, (list, tuple)): # currently no use\n",
    "    sel_proj_id = [proj_id if proj_id.startswith('TCGA-') else 'TCGA-' + proj_id \n",
    "                   for proj_id in sel_proj_id]\n",
    "    sel_proj_list = sel_proj_id\n",
    "\n",
    "data_folder = 'F:/TCGA/GDC13.0'\n",
    "if not os.path.exists(data_folder):\n",
    "  data_folder = '/media/6T/.Trash-1014/GDC13.0'\n",
    "if not os.path.exists(data_folder):\n",
    "  data_folder = '/projects/academic/azhang/tianlema/TCGA/GDC13.0'  \n",
    "data_filepath = f'{data_folder}/htseq_cnt_go_clinical.pkl'\n",
    "# if not os.path.exists(data_filepath):\n",
    "#   print(f'Write file {data_filepath}')\n",
    "#   with open(save_filepath, 'wb') as f:\n",
    "#       pickle.dump({'htseq_cnt_mat': htseq_cnt_mat, 'gene_ids': gene_ids, \n",
    "#                    'aliquot_ids': aliquot_ids, 'go_gene_list': go_gene_list, \n",
    "#                   'go_edges': go_edges, 'string_ppi': ppi, \n",
    "#                   'percent_tumor_nuclei': percent_tumor_nuclei,\n",
    "#                   'sample_type': sample_type, 'diagnoses': diagnoses,\n",
    "#                   'clinical': clinical, 'cases': cases, 'survival_plot': survival_plot}, \n",
    "#                  f)\n",
    "with open(data_filepath, 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "go_gene_list = data['go_gene_list']\n",
    "go_edges = data['go_edges']\n",
    "gene_ids = data['gene_ids']\n",
    "htseq_cnt_mat = data['htseq_cnt_mat']\n",
    "ppi = data['string_ppi']\n",
    "clinical = data['clinical']\n",
    "aliquot_ids = data['aliquot_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if use_jia_data:\n",
    "  print('Use JIA data')\n",
    "  with open(f'{data_folder}/JIA.pkl', 'rb') as f:\n",
    "    jia = pickle.load(f)\n",
    "  gene_cnt = jia['gene_cnt']\n",
    "  sample_info = jia['sample_info']\n",
    "  gene_ids = np.array(gene_cnt.index)\n",
    "  mat = np.log2(gene_cnt.values+1)\n",
    "  type_to_id = {'ADT': 1, 'CRM':2, 'HC':0}\n",
    "  y_true = np.array([type_to_id[s] for s in sample_info['disease.status']])\n",
    "  sel_idx = y_true<2\n",
    "  y_true = y_true[sel_idx]\n",
    "  mat = mat[:, sel_idx]\n",
    "  xs = [mat]\n",
    "  ys = [y_true]\n",
    "else:\n",
    "  print('Use TCGA data')\n",
    "  mat = np.log2(htseq_cnt_mat+1)\n",
    "  # most are from are TCGA projects, only a few are from TARGET\n",
    "  # there are technical duplicates; for simplicity, treat them as different samples\n",
    "  submitter_ids = [s[:12] if s.startswith('TCGA') else s[:16] for s in aliquot_ids]\n",
    "  submitter_set = set(submitter_ids)\n",
    "  sample_submitter_ids = [s[:16] if s.startswith('TCGA') else s[:20] for s in aliquot_ids]\n",
    "  sample_submitter_set = set(sample_submitter_ids)\n",
    "\n",
    "  sample_tumor_stage = {}\n",
    "  diagnoses = data['diagnoses']\n",
    "  # print(list(diagnoses.columns).index('submitter_id'), \n",
    "  #       list(diagnoses.columns).index('tumor_stage'))\n",
    "  for s in diagnoses.values:\n",
    "    if s[9].startswith('TCGA') and s[9].endswith('diagnosis'):\n",
    "      sample_tumor_stage[s[9][:-10]] = s[11]\n",
    "  sample_tumor_stage = {k: v for k, v in sample_tumor_stage.items() if k in submitter_set}\n",
    "  sample_tumor_stage = {k: v for k, v in sample_tumor_stage.items() if v.startswith('stage')}\n",
    "\n",
    "  sample_type = data['sample_type']\n",
    "  common_sample_submitter_ids = set(sample_type.submitter_id).intersection(sample_submitter_ids)\n",
    "  sample_type = {k: v for k, v in zip(sample_type.submitter_id, sample_type.sample_type) \n",
    "                 if k in common_sample_submitter_ids}\n",
    "\n",
    "  # currently not used\n",
    "  percent_tumor_nuclei = data['percent_tumor_nuclei']\n",
    "  percent_tumor_nuclei = {k: v for k, v in zip(percent_tumor_nuclei.submitter_id, \n",
    "                                          percent_tumor_nuclei.percent_tumor_nuclei)\n",
    "                          if k in sample_submitter_set}\n",
    "\n",
    "  submitter_proj_dict = {s: p for s, p in zip(clinical.submitter_id, clinical.project_id)}\n",
    "  proj_case_cnt = collections.Counter([submitter_proj_dict[s] for s in submitter_ids])\n",
    "  # sorted(zip(proj_case_cnt.values(), proj_case_cnt.keys()))\n",
    "  proj_case_loc_list = collections.defaultdict(list)\n",
    "  for i, s in enumerate(submitter_ids):\n",
    "    proj_id = submitter_proj_dict[s]\n",
    "    proj_case_loc_list[proj_id].append(i)\n",
    "#   candidate_proj_list = []\n",
    "#   for k, v in proj_case_loc_list.items():\n",
    "#     if k.startswith('TCGA'):\n",
    "#       tmp = collections.Counter(s[13:15] for s in aliquot_ids[v])\n",
    "#       if tmp['01'] >= 100 and tmp['11'] >= 50:\n",
    "#         print(k, tmp['01'], tmp['11'])\n",
    "#         candidate_proj_list.append(k[5:])\n",
    "#   print(candidate_proj_list)\n",
    "    \n",
    "  if target_name == 'tumor_stage':\n",
    "    tumor_stage_cnt = {}\n",
    "    min_num_per_cls = 100\n",
    "    min_num_per_type = 200\n",
    "    for k, v in proj_case_loc_list.items():\n",
    "      if k.startswith('TCGA') and len(v)>0:\n",
    "        sample_tumor_stage_cnt = collections.Counter(sample_tumor_stage[s]\n",
    "                                              for s in np.array(submitter_ids)[v] \n",
    "                                              if s in sample_tumor_stage)\n",
    "        sample_tumor_stage_cnt = {k: v for k, v in sample_tumor_stage_cnt.items() \n",
    "                                  if v>=min_num_per_cls}\n",
    "        if (len(sample_tumor_stage_cnt)>1 \n",
    "            and sum(sample_tumor_stage_cnt.values())>=min_num_per_type):\n",
    "          tumor_stage_cnt[k] = sample_tumor_stage_cnt\n",
    "#           print(k)\n",
    "#           print(sample_tumor_stage_cnt)\n",
    "#           print()\n",
    "    # only consider TCGA projects\n",
    "    assert isinstance(sel_proj_id, str)\n",
    "    sel_cls = set(tumor_stage_cnt[sel_proj_id])\n",
    "    sel_aliquot_loc = []\n",
    "    y_target = []\n",
    "    for i, s in enumerate(aliquot_ids):\n",
    "      if (submitter_proj_dict[submitter_ids[i]] == sel_proj_id \n",
    "          and submitter_ids[i] in sample_tumor_stage\n",
    "          and sample_tumor_stage[submitter_ids[i]] in sel_cls\n",
    "          and s[13:15] == '01'):\n",
    "        sel_aliquot_loc.append(i)\n",
    "        y_target.append(sample_tumor_stage[submitter_ids[i]])\n",
    "    sel_aliquot_loc = np.array(sel_aliquot_loc)\n",
    "    y_target = np.array(y_target)\n",
    "    y_true, target_name_to_id = discrete_to_id(y_target)\n",
    "    ys = [y_true]\n",
    "    mat = mat[:, sel_aliquot_loc]\n",
    "    xs = [mat]\n",
    "  elif target_name == 'sample_type':\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for proj_id in sel_proj_list:\n",
    "      sel_aliquot_loc = proj_case_loc_list[proj_id]\n",
    "      x = mat[:, sel_aliquot_loc]\n",
    "      sample_type_01_loc = [i for i, s in enumerate(aliquot_ids[sel_aliquot_loc]) if s[13:15]=='01']\n",
    "      sample_type_11_loc = [i for i, s in enumerate(aliquot_ids[sel_aliquot_loc]) if s[13:15]=='11']\n",
    "\n",
    "      x = np.concatenate([x[:, sample_type_01_loc], x[:, sample_type_11_loc]], axis=1)\n",
    "      xs.append(x)\n",
    "      # sample_type_01: primary solid tumor label -> 1\n",
    "      # sample_type_11: solid normal label -> 0\n",
    "      y = [1]*len(sample_type_01_loc) + [0]*len(sample_type_11_loc)\n",
    "      ys.append(y)\n",
    "    mat = np.concatenate(xs, axis=1)\n",
    "    y_true = np.concatenate(ys)\n",
    "  elif target_name == 'pfi' or target_name == 'overall_survival':\n",
    "    with open(f'{data_folder}/clinical_pancan.pkl', 'rb') as f:\n",
    "      clinical_pancan = pickle.load(f)\n",
    "    # Use Pan-Cancer clinical dataset published in Cell\n",
    "    pfi = {}\n",
    "    overall_survival = {}\n",
    "    for sid, p, o in zip(clinical_pancan.bcr_patient_barcode, clinical_pancan.PFI, \n",
    "                         clinical_pancan.OS):\n",
    "      if sid in submitter_set:\n",
    "        if isinstance(p, float) and p==p:\n",
    "          pfi[sid] = int(p)\n",
    "        if isinstance(o, float) and o==o:\n",
    "          overall_survival[sid] = int(o)\n",
    "    clinical_pancan = {'pfi': pfi, 'overall_survival': overall_survival}\n",
    "#     tmp = collections.defaultdict(list)\n",
    "#     for k, v in clinical_pancan[target_name].items():\n",
    "#       tmp[submitter_proj_dict[k]].append(v)\n",
    "#     candidate_proj_list = []\n",
    "#     for k, v in tmp.items():\n",
    "#       if collections.Counter(v)[0]>=100 and collections.Counter(v)[1]>=100:\n",
    "#         print(k, collections.Counter(v)[0], collections.Counter(v)[1])\n",
    "#         candidate_proj_list.append(k[5:])\n",
    "#     print(candidate_proj_list)\n",
    "    sel_aliquot_loc = []\n",
    "    y_target = []\n",
    "    for i, s in enumerate(aliquot_ids):\n",
    "      if (submitter_proj_dict[submitter_ids[i]] in sel_proj_list\n",
    "          and submitter_ids[i] in clinical_pancan[target_name]\n",
    "          and s[13:15] == '01'):\n",
    "        # there are technical duplicates; for simplicity, treat them as different samples\n",
    "        sel_aliquot_loc.append(i)\n",
    "        y_target.append(clinical_pancan[target_name][submitter_ids[i]])\n",
    "    sel_aliquot_loc = np.array(sel_aliquot_loc)\n",
    "    y_target = np.array(y_target)\n",
    "    y_true, target_name_to_id = discrete_to_id(y_target)\n",
    "    ys = [y_true]\n",
    "    mat = mat[:, sel_aliquot_loc]\n",
    "    xs = [mat]\n",
    "\n",
    "num_cls = len(np.unique(y_true))\n",
    "print(sel_proj_id, target_name, mat.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mat.mean(axis=1)\n",
    "std = mat.std(axis=1)\n",
    "idx = np.argsort(-std)[:init_num_gene]\n",
    "gene_ids = gene_ids[idx]\n",
    "mat = mat[idx]\n",
    "print(mat.shape, gene_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_geneset = set(functools.reduce(lambda x,y: x+y, go_gene_list.values()))\n",
    "while common_geneset != set(gene_ids):\n",
    "  common_geneset = common_geneset.intersection(gene_ids)\n",
    "  # remove genes that are not in common_geneset\n",
    "  go_gene_list = {k: sorted(common_geneset.intersection(v)) for k, v in go_gene_list.items()}\n",
    "  # remove GO terms that have less than min_num_gene_per_go genes\n",
    "  go_gene_list = {k: v for k, v in go_gene_list.items() if len(v)>=min_num_gene_per_go}\n",
    "  go_ids = set(go_gene_list)\n",
    "  # some genes may no longer appear in go_gene_list, remove them\n",
    "  common_geneset_new = set(functools.reduce(lambda x,y: x+y, go_gene_list.values()))\n",
    "  idx_gene = np.array([(i, g) for i, g in enumerate(gene_ids) if g in common_geneset_new])\n",
    "  gene_ids = idx_gene[:, 1]\n",
    "  mat = mat[idx_gene[:, 0].astype('int')]\n",
    "  print(gene_ids.shape, mat.shape, len(common_geneset))\n",
    "\n",
    "if len(xs) > 1:\n",
    "  cnt = 0\n",
    "  for i, x in enumerate(xs):\n",
    "    xs[i] = mat[:, cnt:cnt+x.shape[1]]\n",
    "    cnt = cnt + x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 4 # len(chain_graph_go)\n",
    "name_to_id_gene = {n:i for i, n in enumerate(gene_ids)}\n",
    "num_gene = len(gene_ids)\n",
    "\n",
    "num_go = len(go_gene_list)\n",
    "go_ids = set(go_gene_list)\n",
    "go_edges = go_edges[[s[0] in go_ids and s[1] in go_ids for s in go_edges]]\n",
    "name_to_id_go, chain_graph_go = get_topological_order(go_edges[:,[1,0]])\n",
    "print('chain_graph_go', [(i, len(v)) for i, v in enumerate(chain_graph_go)])\n",
    "# there are some isolated GO terms; include them as well in the model\n",
    "for go in go_gene_list:\n",
    "  if go not in name_to_id_go:\n",
    "    name_to_id_go[go] = len(name_to_id_go)\n",
    "# prepare dag\n",
    "dag = collections.defaultdict(list)\n",
    "for s in go_edges:\n",
    "  left = name_to_id_go[s[0]]\n",
    "  right = name_to_id_go[s[1]]\n",
    "  dag[right].append(left)\n",
    "dag = {k: sorted(set(v)) for k, v in dag.items()}\n",
    "# prepare bigraph\n",
    "id_to_name_go = {i: n for n, i in name_to_id_go.items()}\n",
    "num_leaf_go = min(dag)\n",
    "bigraph = []\n",
    "for i in range(num_leaf_go):\n",
    "  bigraph.append(sorted([name_to_id_gene[v] for v in go_gene_list[id_to_name_go[i]]]))\n",
    "print(f'num_go={num_go}, num_gene={num_gene}, num_steps={num_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mats_filepath = f'attention_mats-{num_gene}-{num_go}-{num_steps}-GeneNet.pkl'\n",
    "overwrite = True\n",
    "if overwrite or not os.path.exists(attention_mats_filepath):\n",
    "#   gene_gene_adj_mat, _ = adj_list_to_mat(ppi, name_to_id=name_to_id_gene, bipartite=False, \n",
    "#                                        add_self_loop=True, symmetric=True, return_list=False)\n",
    "  gene_gene_adj_mat = np.zeros((num_gene, num_gene))\n",
    "  for s in ppi:\n",
    "    if s[0] in name_to_id_gene and s[1] in name_to_id_gene:\n",
    "      left = name_to_id_gene[s[0]]\n",
    "      right = name_to_id_gene[s[1]]\n",
    "      gene_gene_adj_mat[left, right] = float(s[2])/1000\n",
    "  gene_gene_adj_mat[range(num_gene), range(num_gene)] = 1\n",
    "  \n",
    "  go_go_adj_mat, _ = adj_list_to_mat(go_edges, name_to_id=name_to_id_go, bipartite=False, \n",
    "                   add_self_loop=False, symmetric=False, return_list=False)\n",
    "  \n",
    "  gene_go_adj_mat = np.zeros((num_gene, num_go))\n",
    "  for k, v in go_gene_list.items():\n",
    "    go_id = name_to_id_go[k]\n",
    "    for g in v:\n",
    "      gene_id = name_to_id_gene[g]\n",
    "      gene_go_adj_mat[gene_id, go_id] = 1\n",
    "      \n",
    "  attention_mats = {}\n",
    "  start_time = time.time()\n",
    "  attention_mats['gene1->gene0'], id_to_name_gene = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps, name_to_id=name_to_id_gene, bipartite=False, \n",
    "    add_self_loop=True, symmetric=True, target_to_source=None, use_transition_matrix=True, \n",
    "    Ms=gene_gene_adj_mat, softmax_normalization=False, min_value=-100, device=device)\n",
    "\n",
    "  attention_mats['pathway1->pathway0'], id_to_name_go = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps, name_to_id=name_to_id_go, bipartite=False, \n",
    "    add_self_loop=False, symmetric=False, target_to_source=None, use_transition_matrix=True, \n",
    "    Ms=go_go_adj_mat, softmax_normalization=False, min_value=-100, device=device)\n",
    "\n",
    "  mats, _ = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps*2, name_to_id=[name_to_id_gene, name_to_id_go], \n",
    "    bipartite=True, add_self_loop=False, symmetric=False, target_to_source=None, \n",
    "    use_transition_matrix=True, Ms=gene_go_adj_mat, softmax_normalization=False, min_value=-100, \n",
    "    device=device)\n",
    "  # this is very tricky: \n",
    "  # the even positions are all gene->pathway in mats[0], while odd ones gene->gene\n",
    "  attention_mats['gene0->pathway1'] = [m for i, m in enumerate(mats[0]) if i%2==0]\n",
    "  attention_mats['pathway0->gene1'] = [m for i, m in enumerate(mats[1]) if i%2==0]\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(f'Time spent on generating attention_mats {end_time - start_time} s')\n",
    "\n",
    "#   start_time = time.time()\n",
    "#   with open(attention_mats_filepath, 'wb') as f:\n",
    "#     if device == torch.device('cuda'):\n",
    "#       for k, v in attention_mats.items():\n",
    "#         for i in range(len(v)):\n",
    "#           v[i] = v[i].detach().cpu().numpy()\n",
    "#     pickle.dump(attention_mats, f)\n",
    "#   end_time = time.time()\n",
    "#   print(f'Time spent on writing attention_mats {end_time - start_time} s')\n",
    "\n",
    "# start_time = time.time()\n",
    "# with open('attention_mats-GeneNet.pkl', 'rb') as f:\n",
    "#   attention_mats = pickle.load(f)\n",
    "#   for k, v in attention_mats.items():\n",
    "#     for i in range(len(v)):\n",
    "#       v[i] = torch.tensor(v[i]).float().to(device)\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on loading attention_mats {end_time - start_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_rows(xs, seed=None, randperm=None):\n",
    "  if seed is not None:\n",
    "    torch.random.manual_seed(seed)\n",
    "  if isinstance(xs, (list, tuple)):\n",
    "    if randperm is None:\n",
    "      randperm = torch.randperm(len(xs[0]))\n",
    "    for i, x in enumerate(xs):\n",
    "      assert len(x)==len(xs[0])\n",
    "      xs[i] = x[randperm]\n",
    "  else:\n",
    "    if randperm is None:\n",
    "      randperm = torch.randperm(len(xs))\n",
    "    xs = xs[randperm]\n",
    "  return xs\n",
    "\n",
    "if len(xs) == 1:\n",
    "  x_all = torch.tensor(mat.T).float().to(device)\n",
    "  y_all = torch.tensor(y_true).long().to(device)\n",
    "  # this permutation is needed because later we sequentially split them into several buckets\n",
    "  x_all, y_all = permute_rows([x_all, y_all], seed=seed)\n",
    "  if randomize_labels:\n",
    "    # y_all = permute_rows(y_all)\n",
    "    y_all = torch.randint(2, y_all.size()).long().to(device)\n",
    "\n",
    "  cls_loc = collections.defaultdict(list)\n",
    "  for i, e in enumerate(y_all):\n",
    "    cls_loc[e.item()].append(i)\n",
    "\n",
    "  num_split = len(split_portion)\n",
    "  preset_split_size = None\n",
    "  split_loc = collections.defaultdict(list)\n",
    "  for c, v in cls_loc.items():\n",
    "    # split_portion and split_size have been defined before\n",
    "    split_size = get_split(len(v), split_portion, split_size=preset_split_size)\n",
    "    cnt = 0\n",
    "    for s in split_size:\n",
    "      split_loc[c].append(v[cnt:cnt+s])\n",
    "      cnt = cnt + s\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for i in range(num_split):\n",
    "    x_split = []\n",
    "    y_split = []\n",
    "    for c, v in sorted(split_loc.items()):\n",
    "      x_split.append(x_all[v[i]])\n",
    "      y_split.append(y_all[v[i]])\n",
    "    xs.append(torch.cat(x_split, dim=0))\n",
    "    ys.append(torch.cat(y_split, dim=0))\n",
    "else:\n",
    "  xs = [torch.tensor(x.T).float().to(device) for x in xs]\n",
    "  ys = [torch.tensor(y).long().to(device) for y in ys]\n",
    "\n",
    "x_train, x_val, x_test = xs[:3]\n",
    "y_train, y_val, y_test = ys[:3]\n",
    "  \n",
    "# permute rows to remove the location dependency for data points of the same classes\n",
    "x_train, y_train = permute_rows([x_train, y_train], seed=seed)\n",
    "x_val, y_val = permute_rows([x_val, y_val], seed=seed)\n",
    "x_test, y_test = permute_rows([x_test, y_test], seed=seed)\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "plot_scatter(y_=x_train, colors=['r' if i==1 else 'g' for i in y_train], title='train')\n",
    "plot_scatter(y_=x_val, colors=['r' if i==1 else 'g' for i in y_val], title='val')\n",
    "plot_scatter(y_=x_test, colors=['r' if i==1 else 'g' for i in y_test], title='test')\n",
    "\n",
    "model_names = []\n",
    "split_names = ['train', 'val', 'test']\n",
    "metric_names = ['acc', 'precision', 'recall', 'f1_score', 'adjusted_mutual_info', 'auc', \n",
    "                'average_precision']\n",
    "metric_all = []\n",
    "confusion_mat_all = []\n",
    "loss_his_all = []\n",
    "acc_his_all = []\n",
    "\n",
    "loss_fn_cls = nn.CrossEntropyLoss()\n",
    "loss_fn_reg = nn.MSELoss()\n",
    "if num_cls == 1:\n",
    "  loss_fn = loss_fn_reg\n",
    "else:\n",
    "  loss_fn = loss_fn_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneNet\n",
    "model_names.append('GeneNet')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "use_dag_layer = False\n",
    "dag_in_channel_list = [1,1]\n",
    "dag_kwargs = {'residual':True, 'duplicate_dag':True}\n",
    "\n",
    "forward_kwargs = {'attention_mats': attention_mats, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = GeneNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "                use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "                dag_kwargs=dag_kwargs, nonlinearity=nn.ReLU(), use_layer_norm=True,\n",
    "               num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PathNet: use GO gene set and GO hierarchy\n",
    "model_names.append('PathNet')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "attention_mats_pathnet = {'pathway0->gene': attention_mats['pathway0->gene1'],\n",
    "                 'pathway1->pathway0': attention_mats['pathway1->pathway0'],\n",
    "                 'gene->pathway1': attention_mats['gene0->pathway1']}\n",
    "\n",
    "forward_kwargs = {'attention_mats': attention_mats_pathnet, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = PathNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "                use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "                dag_kwargs=dag_kwargs, nonlinearity=nn.ReLU(), use_layer_norm=True,\n",
    "               num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipartiteGraph \n",
    "model_names.append('BipartiteGraph')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "# mats had been calculated when we first calculate attention_mats\n",
    "forward_kwargs = {'attention_mats': mats, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = BipartiteGraph1d(in_features=num_gene, out_features=num_go, \n",
    "                         use_layer_norm=True, num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "go_weight_bipartite_graph = model.classifier.weight.detach().cpu().numpy()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "model_names.append('MLP')\n",
    "print(f'{model_names[-1]}')\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "hidden_dim = [100]\n",
    "dense = False\n",
    "residual = False\n",
    "\n",
    "model = DenseLinear(in_dim, hidden_dim+[num_cls], dense=dense, residual=residual).to(device)\n",
    "multi_heads = False\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "train_result = train_single_loss(model,  \n",
    "                    x_train, y_train if num_cls>1 else y_train.unsqueeze(-1).float(), \n",
    "                    x_val, y_val if num_cls>1 else y_val.unsqueeze(-1).float(), \n",
    "                    x_test, y_test if num_cls>1 else y_test.unsqueeze(-1).float(), \n",
    "                    loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, \n",
    "    reduce_every=reduce_every, eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val)\n",
    "if train_result is not None:\n",
    "  best_model, best_val_acc, best_epoch = train_result\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, \n",
    "                    x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                    batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "        show_results_in_notebook=True, loss_idx=0, acc_idx=0, forward_kwargs={}, \n",
    "        predict_func=None, pred_kwargs=None, plot_loss=True, \n",
    "        plot_acc=True if num_cls>1 else False, plot_scatter=True if num_cls>1 else False, \n",
    "        loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "        acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrototypicalNet, requires_grad_prototype=False\n",
    "model_names.append('PrototypicalNet0')\n",
    "print(f'{model_names[-1]}')\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "hidden_dim = [100]\n",
    "dense = False\n",
    "residual = False\n",
    "\n",
    "model = PrototypicalNet(in_dim, hidden_dim, num_cls, dense=dense, \n",
    "                        residual=residual, requires_grad_prototype=False).to(device)\n",
    "multi_heads = False\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# must provide forward_kwargs_train for PrototypicalNet; \n",
    "# all other things are the same as DenseLinear\n",
    "train_result = train_single_loss(model,  \n",
    "                    x_train, y_train if num_cls>1 else y_train.unsqueeze(-1).float(), \n",
    "                    x_val, y_val if num_cls>1 else y_val.unsqueeze(-1).float(), \n",
    "                    x_test, y_test if num_cls>1 else y_test.unsqueeze(-1).float(), \n",
    "                    loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, \n",
    "    reduce_every=reduce_every, eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train={'y': y_train, 'train': True})\n",
    "if train_result is not None:\n",
    "  best_model, best_val_acc, best_epoch = train_result\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, \n",
    "                    x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                    batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "        show_results_in_notebook=True, loss_idx=0, acc_idx=0, forward_kwargs={}, \n",
    "        predict_func=None, pred_kwargs=None, plot_loss=True, \n",
    "        plot_acc=True if num_cls>1 else False, plot_scatter=True if num_cls>1 else False, \n",
    "        loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "        acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrototypicalNet, requires_grad_prototype=True\n",
    "model_names.append('PrototypicalNet1')\n",
    "print(f'{model_names[-1]}')\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "hidden_dim = [100]\n",
    "dense = False\n",
    "residual = False\n",
    "\n",
    "model = PrototypicalNet(in_dim, hidden_dim, num_cls, dense=dense, \n",
    "                        residual=residual, requires_grad_prototype=True).to(device)\n",
    "multi_heads = False\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# must provide forward_kwargs_train for PrototypicalNet; \n",
    "# all other things are the same as DenseLinear\n",
    "train_result = train_single_loss(model,  \n",
    "                    x_train, y_train if num_cls>1 else y_train.unsqueeze(-1).float(), \n",
    "                    x_val, y_val if num_cls>1 else y_val.unsqueeze(-1).float(), \n",
    "                    x_test, y_test if num_cls>1 else y_test.unsqueeze(-1).float(), \n",
    "                    loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, \n",
    "    reduce_every=reduce_every, eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train={'y': y_train, 'train': True})\n",
    "if train_result is not None:\n",
    "  best_model, best_val_acc, best_epoch = train_result\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, \n",
    "                    x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                    batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "        show_results_in_notebook=True, loss_idx=0, acc_idx=0, forward_kwargs={}, \n",
    "        predict_func=None, pred_kwargs=None, plot_loss=True, \n",
    "        plot_acc=True if num_cls>1 else False, plot_scatter=True if num_cls>1 else False, \n",
    "        loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "        acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DAGEncoder\n",
    "# model_names.append('DAG')\n",
    "# print(f'{model_names[-1]} Model')\n",
    "\n",
    "# embedding_dim = 5\n",
    "# in_channels_list = [5]\n",
    "# key_dim = 5\n",
    "# value_dim = 5\n",
    "# fc_dim = 5\n",
    "# dim_per_cls = 1\n",
    "# num_heads = 1\n",
    "# num_attention = 1\n",
    "# use_encoders = False\n",
    "# # num_go_dag = max(dag)+1\n",
    "# # print(num_go_dag)\n",
    "# graph_encoder = None # [m[:num_go_dag][:,:num_go_dag] for m in attention_mats['pathway1->pathway0']]\n",
    "# graph_weight_encoder = 0.5\n",
    "# knn = 50\n",
    "\n",
    "# residual = True \n",
    "# duplicate_dag = True \n",
    "# gibbs_sampling = True \n",
    "# duplicated_attention = True \n",
    "# feature_max_norm = 1\n",
    "# use_layer_norm = True \n",
    "# bias = True \n",
    "# nonlinearity = nn.ReLU()\n",
    "\n",
    "# forward_kwargs = {'return_attention': False, 'graph_encoder': None, 'graph_decoder': None, \n",
    "#     'encoder_stochastic_depth': False}\n",
    "\n",
    "# model = DAGEncoder(num_features=num_gene, embedding_dim=embedding_dim, \n",
    "#          in_channels_list=in_channels_list, \n",
    "#          bigraph=bigraph, dag=dag, key_dim=key_dim, value_dim=value_dim, fc_dim=fc_dim, \n",
    "#          num_cls=num_cls, dim_per_cls=dim_per_cls, feature_max_norm=feature_max_norm, \n",
    "#          use_layer_norm=use_layer_norm, bias=bias, \n",
    "#          nonlinearity=nonlinearity, residual=residual, duplicate_dag=duplicate_dag, \n",
    "#          gibbs_sampling=gibbs_sampling, num_heads=num_heads, num_attention=num_attention,\n",
    "#          knn=knn, duplicated_attention=duplicated_attention,\n",
    "#          graph_encoder=None, graph_weight_encoder=graph_weight_encoder, \n",
    "#          graph_decoder=None, graph_weight_decoder=0.5, use_encoders=use_encoders).to(device)\n",
    "\n",
    "# start_time = time.time()\n",
    "# y_pred = model(x_train, **forward_kwargs)\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "# start_time = time.time()\n",
    "# loss = loss_fn(y_pred, y_train)\n",
    "# loss.backward()\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "# loss_train_his = []\n",
    "# loss_val_his = []\n",
    "# loss_test_his = []\n",
    "# acc_train_his = []\n",
    "# acc_val_his = []\n",
    "# acc_test_his = []\n",
    "# best_model = model\n",
    "# best_val_acc = 0\n",
    "# best_epoch = 0\n",
    "\n",
    "# best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "#     x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "#     amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "#     eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "#     loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "#     acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "#     return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "#     forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "\n",
    "# metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "#             x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "#             show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "#             forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "#             plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "#             loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "#             acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "# loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "# acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "# metric_all.append([v[0] for v in metric])\n",
    "# confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphConvolutionNetwork\n",
    "model_names.append('GCN')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "use_string_ppi = True\n",
    "if use_string_ppi:\n",
    "  attention_mat_gcn = attention_mats['gene1->gene0'][0]\n",
    "else:\n",
    "  attention_mat_gcn = attention_mats['gene1->gene0'][1]\n",
    "attention_mat_gcn = (attention_mat_gcn + attention_mat_gcn.t())/2\n",
    "  \n",
    "duplicate_layers=False, \n",
    "dense=False \n",
    "residual=True\n",
    "use_bias=True\n",
    "use_layer_norm=False \n",
    "nonlinearity=nn.ReLU()\n",
    "classifier_bias=True\n",
    "\n",
    "forward_kwargs = {'attention_mats':attention_mat_gcn, 'max_num_layers':num_steps, \n",
    "                  'min_num_layers':num_steps, 'return_layers': 'last-layer'}\n",
    "\n",
    "model = GraphConvolution1d(num_features=num_gene, num_layers=num_steps, \n",
    "          duplicate_layers=duplicate_layers, dense=dense, residual=residual, use_bias=use_bias, \n",
    "          use_layer_norm=use_layer_norm, nonlinearity=nonlinearity, num_cls=num_cls, \n",
    "          classifier_bias=classifier_bias).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "model_names_sklearn = ['kNN', 'Naive Bayes', 'SVM', 'Decision Tree', \n",
    "                             'Random Forest', 'AdaBoost']\n",
    "model_names = list(model_names) + model_names_sklearn\n",
    "\n",
    "classifiers = [KNeighborsClassifier(5), \n",
    "               GaussianNB(), \n",
    "               sklearn.svm.SVC(kernel=\"linear\", C=0.025),\n",
    "               DecisionTreeClassifier(max_depth=5),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10),\n",
    "               AdaBoostClassifier()\n",
    "              ]\n",
    "\n",
    "# assert train_portion > 0 and val_portion > 0 and test_portion > 0 # Assume there are 3 splits\n",
    "for name, classifier in zip(model_names_sklearn, classifiers):\n",
    "  print(name)\n",
    "  classifier.fit(x_train, y_train)\n",
    "  metric = []\n",
    "  for x_, y_ in zip([x_train, x_val, x_test], [y_train, y_val, y_test]):\n",
    "    if name == 'SVM':\n",
    "      y_score = classifier.decision_function(x_) # sklearn.svm.SVC does not have predict_proba\n",
    "    else:\n",
    "      y_score = classifier.predict_proba(x_)\n",
    "    metric.append(eval_classification(y_true=y_, y_pred=y_score, \n",
    "                                      average='weighted', verbose=True))\n",
    "  metric_all.append([v[0] for v in metric])\n",
    "  confusion_mat_all.append([v[1] for v in metric])\n",
    "  # loss and accuracy history are discarded for these classifiers\n",
    "  loss_his_all.append([])\n",
    "  acc_his_all.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize the order of genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx_feature = torch.randperm(x_train.size(1))\n",
    "x_train = x_train[:, random_idx_feature]\n",
    "x_val = x_val[:, random_idx_feature]\n",
    "x_test = x_test[:, random_idx_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneNet\n",
    "model_names.append('RandGeneNet')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "use_dag_layer = False\n",
    "dag_in_channel_list = [1,1]\n",
    "dag_kwargs = {'residual':True, 'duplicate_dag':True}\n",
    "\n",
    "forward_kwargs = {'attention_mats': attention_mats, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = GeneNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "                use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "                dag_kwargs=dag_kwargs, nonlinearity=nn.ReLU(), use_layer_norm=True,\n",
    "               num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PathNet: use GO gene set and GO hierarchy\n",
    "model_names.append('RandPathNet')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "attention_mats_pathnet = {'pathway0->gene': attention_mats['pathway0->gene1'],\n",
    "                 'pathway1->pathway0': attention_mats['pathway1->pathway0'],\n",
    "                 'gene->pathway1': attention_mats['gene0->pathway1']}\n",
    "\n",
    "forward_kwargs = {'attention_mats': attention_mats_pathnet, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = PathNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "                use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "                dag_kwargs=dag_kwargs, nonlinearity=nn.ReLU(), use_layer_norm=True,\n",
    "               num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipartiteGraph \n",
    "model_names.append('RandBipartiteGraph')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "# mats had been calculated when we first calculate attention_mats\n",
    "forward_kwargs = {'attention_mats': mats, 'max_num_layers': num_steps, \n",
    "          'min_num_layers': num_steps, 'return_layers': 'cls_score'}\n",
    "\n",
    "model = BipartiteGraph1d(in_features=num_gene, out_features=num_go, \n",
    "                         use_layer_norm=True, num_cls=num_cls).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print('Before training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "print('After training: last layer weight distribution')\n",
    "plt.plot(sorted(model.classifier.weight[0].detach().cpu().numpy()))\n",
    "plt.show()\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "model_names.append('RandMLP')\n",
    "print(f'{model_names[-1]}')\n",
    "\n",
    "in_dim = x_train.shape[1]\n",
    "hidden_dim = [100]\n",
    "dense = False\n",
    "residual = False\n",
    "\n",
    "model = DenseLinear(in_dim, hidden_dim+[num_cls], dense=dense, residual=residual).to(device)\n",
    "multi_heads = False\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "train_result = train_single_loss(model,  \n",
    "                    x_train, y_train if num_cls>1 else y_train.unsqueeze(-1).float(), \n",
    "                    x_val, y_val if num_cls>1 else y_val.unsqueeze(-1).float(), \n",
    "                    x_test, y_test if num_cls>1 else y_test.unsqueeze(-1).float(), \n",
    "                    loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, \n",
    "    reduce_every=reduce_every, eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val)\n",
    "if train_result is not None:\n",
    "  best_model, best_val_acc, best_epoch = train_result\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, \n",
    "                    x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                    batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "        show_results_in_notebook=True, loss_idx=0, acc_idx=0, forward_kwargs={}, \n",
    "        predict_func=None, pred_kwargs=None, plot_loss=True, \n",
    "        plot_acc=True if num_cls>1 else False, plot_scatter=True if num_cls>1 else False, \n",
    "        loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "        acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DAGEncoder\n",
    "# model_names.append('RandDAG')\n",
    "# print(f'{model_names[-1]} Model')\n",
    "\n",
    "# embedding_dim = 5\n",
    "# in_channels_list = [5]\n",
    "# key_dim = 5\n",
    "# value_dim = 5\n",
    "# fc_dim = 5\n",
    "# dim_per_cls = 1\n",
    "# num_heads = 1\n",
    "# num_attention = 1\n",
    "# num_go = max(dag)+1\n",
    "# print(num_go)\n",
    "# use_encoders = False\n",
    "# graph_encoder = None # [m[:num_go][:,:num_go] for m in attention_mats['pathway1->pathway0']]\n",
    "# graph_weight_encoder = 0.5\n",
    "# knn = 50\n",
    "\n",
    "# residual = True \n",
    "# duplicate_dag = True \n",
    "# gibbs_sampling = True \n",
    "# duplicated_attention = True \n",
    "# feature_max_norm = 1\n",
    "# use_layer_norm = True \n",
    "# bias = True \n",
    "# nonlinearity = nn.ReLU()\n",
    "\n",
    "# forward_kwargs = {'return_attention': False, 'graph_encoder': None, 'graph_decoder': None, \n",
    "#     'encoder_stochastic_depth': False}\n",
    "\n",
    "# model = DAGEncoder(num_features=num_gene, embedding_dim=embedding_dim, \n",
    "#          in_channels_list=in_channels_list, \n",
    "#          bigraph=bigraph, dag=dag, key_dim=key_dim, value_dim=value_dim, fc_dim=fc_dim, \n",
    "#          num_cls=num_cls, dim_per_cls=dim_per_cls, feature_max_norm=feature_max_norm, \n",
    "#          use_layer_norm=use_layer_norm, bias=bias, \n",
    "#          nonlinearity=nonlinearity, residual=residual, duplicate_dag=duplicate_dag, \n",
    "#          gibbs_sampling=gibbs_sampling, num_heads=num_heads, num_attention=num_attention,\n",
    "#          knn=knn, duplicated_attention=duplicated_attention,\n",
    "#          graph_encoder=None, graph_weight_encoder=graph_weight_encoder, \n",
    "#          graph_decoder=None, graph_weight_decoder=0.5, use_encoders=use_encoders).to(device)\n",
    "\n",
    "# start_time = time.time()\n",
    "# y_pred = model(x_train, **forward_kwargs)\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "# start_time = time.time()\n",
    "# loss = loss_fn(y_pred, y_train)\n",
    "# loss.backward()\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "# loss_train_his = []\n",
    "# loss_val_his = []\n",
    "# loss_test_his = []\n",
    "# acc_train_his = []\n",
    "# acc_val_his = []\n",
    "# acc_test_his = []\n",
    "# best_model = model\n",
    "# best_val_acc = 0\n",
    "# best_epoch = 0\n",
    "\n",
    "# best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "#     x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "#     amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "#     eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "#     loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "#     acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "#     return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "#     forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "\n",
    "# metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "#             x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "#             show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "#             forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "#             plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "#             loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "#             acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "# loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "# acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "# metric_all.append([v[0] for v in metric])\n",
    "# confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphConvolutionNetwork\n",
    "model_names.append('RandGCN')\n",
    "print(f'{model_names[-1]} Model')\n",
    "\n",
    "use_string_ppi = True\n",
    "if use_string_ppi:\n",
    "  attention_mat_gcn = attention_mats['gene1->gene0'][0]\n",
    "else:\n",
    "  attention_mat_gcn = attention_mats['gene1->gene0'][1]\n",
    "attention_mat_gcn = (attention_mat_gcn + attention_mat_gcn.t())/2\n",
    "  \n",
    "duplicate_layers=False, \n",
    "dense=False \n",
    "residual=True\n",
    "use_bias=True\n",
    "use_layer_norm=False \n",
    "nonlinearity=nn.ReLU()\n",
    "classifier_bias=True\n",
    "\n",
    "forward_kwargs = {'attention_mats':attention_mat_gcn, 'max_num_layers':num_steps, \n",
    "                  'min_num_layers':num_steps, 'return_layers': 'last-layer'}\n",
    "\n",
    "model = GraphConvolution1d(num_features=num_gene, num_layers=num_steps, \n",
    "          duplicate_layers=duplicate_layers, dense=dense, residual=residual, use_bias=use_bias, \n",
    "          use_layer_norm=use_layer_norm, nonlinearity=nonlinearity, num_cls=num_cls, \n",
    "          classifier_bias=classifier_bias).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = model(x_train, **forward_kwargs)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = loss_fn(y_pred, y_train)\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')\n",
    "\n",
    "\n",
    "loss_train_his = []\n",
    "loss_val_his = []\n",
    "loss_test_his = []\n",
    "acc_train_his = []\n",
    "acc_val_his = []\n",
    "acc_test_his = []\n",
    "best_model = model\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "best_model, best_val_acc, best_epoch = train_single_loss(model, x_train, y_train, \n",
    "    x_val, y_val, x_test, y_test, loss_fn=loss_fn, lr=lr, weight_decay=weight_decay, \n",
    "    amsgrad=True, batch_size=batch_size, num_epochs=num_epochs, reduce_every=reduce_every, \n",
    "    eval_every=eval_every, print_every=print_every, verbose=False, \n",
    "    loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his, \n",
    "    acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his, \n",
    "    return_best_val=return_best_val, forward_kwargs_train=forward_kwargs, \n",
    "    forward_kwargs_val=forward_kwargs, forward_kwargs_test=forward_kwargs)\n",
    "\n",
    "metric = get_result(model, best_model, best_val_acc, best_epoch, x_train, y_train, x_val, y_val, \n",
    "            x_test, y_test, batch_size=batch_size, multi_heads=False, average='weighted', \n",
    "            show_results_in_notebook=True, loss_idx=0, acc_idx=0, \n",
    "            forward_kwargs=forward_kwargs, predict_func=None, pred_kwargs=None, \n",
    "            plot_loss=True, plot_acc=True, plot_scatter=False, \n",
    "            loss_train_his=loss_train_his, loss_val_his=loss_val_his, loss_test_his=loss_test_his,\n",
    "            acc_train_his=acc_train_his, acc_val_his=acc_val_his, acc_test_his=acc_test_his)\n",
    "\n",
    "loss_his_all.append([loss_train_his, loss_val_his, loss_test_his])\n",
    "acc_his_all.append([acc_train_his, acc_val_his, acc_test_his])\n",
    "metric_all.append([v[0] for v in metric])\n",
    "confusion_mat_all.append([v[1] for v in metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result comparison\n",
    "model_names_all = np.array(model_names)\n",
    "res_all = np.array(metric_all)\n",
    "\n",
    "metric_idx = 4\n",
    "split_idx = 2\n",
    "# i = 5\n",
    "# subset = range(6*i, 6*i+6)\n",
    "# subset = range(i, len(model_names_all), 6)\n",
    "subset = range(len(model_names_all))\n",
    "if res_all.ndim==4:\n",
    "  mean = res_all.mean(axis=0)[subset]\n",
    "  std = res_all.std(axis=0)[subset]\n",
    "else:\n",
    "  mean = res_all[subset]\n",
    "  std = np.zeros_like(mean)\n",
    "model_names = model_names_all[subset]\n",
    "sorted_idx = np.argsort(-mean, axis=0)\n",
    "mean = mean[sorted_idx[:, split_idx, metric_idx], split_idx, metric_idx]\n",
    "std = std[sorted_idx[:, split_idx, metric_idx], split_idx, metric_idx]\n",
    "names = model_names[sorted_idx[:, split_idx, metric_idx]]\n",
    "res = [(i+1, n, m, sd) for i, (n, m, sd) in enumerate(zip(names, mean, std))]\n",
    "print('{:^4} {:^50} {:^5} \\t {:^5}'.format('Rank', 'Name', 'Mean', 'Std'))\n",
    "for s in res:\n",
    "  print(f'{s[0]:^4} {s[1]:^50} {s[2]:^.3f} \\t {s[3]:^.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_folder}/{res_filename}', 'wb') as f:\n",
    "  print(f'Write result to file {result_folder}/{res_filename}')\n",
    "  pickle.dump({'loss_his_all': loss_his_all,\n",
    "               'acc_his_all': acc_his_all,\n",
    "               'metric_all': metric_all,\n",
    "               'confusion_mat_all': confusion_mat_all,\n",
    "               'model_names': model_names,\n",
    "               'split_names': split_names,\n",
    "               'metric_names': metric_names,\n",
    "               'name_to_id_gene': name_to_id_gene, \n",
    "               'name_to_id_go': name_to_id_go, \n",
    "               'dag': dag, \n",
    "               'bigraph': bigraph, \n",
    "               'go_weight_bipartite_graph': go_weight_bipartite_graph\n",
    "              }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_dag_layer = True\n",
    "# dag_in_channel_list = [1]\n",
    "# dag_kwargs = {'residual':True, 'duplicate_dag':True}\n",
    "# batch_size = 500\n",
    "# model = GeneNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "#                 use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "#                 dag_kwargs=dag_kwargs,\n",
    "#                 nonlinearity=nn.ReLU(), use_layer_norm=True).to(device)\n",
    "\n",
    "# x = torch.randn(batch_size, num_gene).to(device)\n",
    "\n",
    "# start_time = time.time()\n",
    "# y = model(x, attention_mats=attention_mats, max_num_layers=num_steps, min_num_layers=num_steps, \n",
    "#           return_layers='all')\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "# print(y[0].shape, y[1].shape, y[2].shape, y[3].shape)\n",
    "\n",
    "# start_time = time.time()\n",
    "# loss = y[0].sum() + y[1].sum() + y[2].sum() + y[3].sum()\n",
    "# loss.backward()\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on backward pass {end_time - start_time} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
