{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import collections\n",
    "import time\n",
    "import nbformat\n",
    "import socket\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import torch\n",
    "\n",
    "lib_path = 'I:/code'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/media/6T/.tianle/.lib'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/projects/academic/azhang/tianlema/lib'\n",
    "if os.path.exists(lib_path) and lib_path not in sys.path:\n",
    "  sys.path.append(lib_path)\n",
    "  \n",
    "from dl.utils.visualization.visualization import *\n",
    "from dl.utils.train import eval_classification, get_label_prob\n",
    "from dl.utils.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(model_type='nn', dense=False, residual=True, hidden_dim=[100, 100], \n",
    "               train_portion=0.7, val_portion=0.1, test_portion=0.2, \n",
    "               num_sets=10, num_folds=10, sel_set_idx=0,\n",
    "               num_train_types=-1, \n",
    "               num_val_types=-1,\n",
    "               num_test_types=-1,\n",
    "               cv_type='instance-shuffle',\n",
    "               sel_disease_types='all', \n",
    "               min_num_samples_per_type_cls=[100, 0],\n",
    "               predefined_sample_set_file='auto-search',\n",
    "               target_variable='PFI',\n",
    "               target_variable_type='discrete',\n",
    "               target_variable_range=[0, 1],\n",
    "               data_type=['gene', 'mirna', 'methy', 'rppa'], \n",
    "               additional_vars=[],#['age_at_initial_pathologic_diagnosis', 'gender']\n",
    "               additional_var_types=[],#['continuous', 'discrete']\n",
    "               additional_var_ranges=[],\n",
    "               normal_transform_feature=True, \n",
    "               randomize_labels=False,\n",
    "               lr=5e-4,\n",
    "               weight_decay=1e-4,\n",
    "               num_epochs=1000,\n",
    "               reduce_every=500,\n",
    "               show_results_in_notebook=True, \n",
    "               idx_folder='results/data_split_idx', # no longer used\n",
    "               notebook_folder='.', \n",
    "               template_file='exp_template.ipynb', \n",
    "               slurm_script='../gpu-slurm', \n",
    "               new_file=True, submit=True,\n",
    "               cell_idx=2, gpu_id=3):\n",
    "  \"\"\"Create notebook and run it on dlm or submit to ccr slurm\n",
    "  \"\"\"\n",
    "  # This is for filename\n",
    "  sel_disease_type_str = sel_disease_types # will be overwritten if it is a list\n",
    "  if isinstance(sel_disease_types, (list, tuple)):\n",
    "    sel_disease_type_str = '-'.join(sorted(sel_disease_types))\n",
    "  if isinstance(data_type, str):\n",
    "    data_type_str = data_type\n",
    "  else:\n",
    "    data_type_str = '-'.join(sorted(data_type))\n",
    "  if model_type == 'nn': # model_type, dense, residual are dependent\n",
    "    assert not (residual and dense)\n",
    "    if residual:\n",
    "      model_type = 'resnet' \n",
    "    if dense:\n",
    "      model_type = 'densenet'\n",
    "  \n",
    "  args = {'model_type': model_type, # model_type may be different from the argument\n",
    "          'dense': dense,\n",
    "          'residual': residual,\n",
    "          'hidden_dim': hidden_dim,\n",
    "          'train_portion': train_portion,\n",
    "          'val_portion': val_portion,\n",
    "          'test_portion': test_portion,\n",
    "          'num_sets': num_sets,\n",
    "          'num_folds': num_folds,\n",
    "          'num_train_types': num_train_types, \n",
    "          'num_val_types': num_val_types,\n",
    "          'num_test_types': num_test_types,\n",
    "          'cv_type': cv_type,\n",
    "          'sel_set_idx': sel_set_idx,\n",
    "          'sel_disease_types': sel_disease_types,\n",
    "          'min_num_samples_per_type_cls': min_num_samples_per_type_cls,\n",
    "          'predefined_sample_set_file': predefined_sample_set_file,\n",
    "          'target_variable': target_variable,\n",
    "          'target_variable_type': target_variable_type,\n",
    "          'target_variable_range': target_variable_range,\n",
    "          'data_type': data_type,\n",
    "          'additional_vars': additional_vars,#['age_at_initial_pathologic_diagnosis', 'gender']\n",
    "          'additional_var_types': additional_var_types,#['continuous', 'discrete']\n",
    "          'additional_var_ranges': additional_var_ranges,\n",
    "          'normal_transform_feature': normal_transform_feature,\n",
    "          'randomize_labels': randomize_labels,\n",
    "          'lr': lr,\n",
    "          'weight_decay': weight_decay,\n",
    "          'num_epochs': num_epochs,\n",
    "          'reduce_every': reduce_every,\n",
    "          'show_results_in_notebook': show_results_in_notebook\n",
    "         }\n",
    "  \n",
    "  predefined_sample_set_filename = (target_variable if isinstance(target_variable,str) \n",
    "                                else '-'.join(target_variable))\n",
    "  predefined_sample_set_filename += f'_{cv_type}'\n",
    "  if len(additional_vars) > 0:\n",
    "    predefined_sample_set_filename += f\"_{'-'.join(sorted(additional_vars))}\"\n",
    "  predefined_sample_set_filename += (f\"_{data_type_str}_{sel_disease_type_str}_\"\n",
    "                                     f\"{'-'.join(map(str, min_num_samples_per_type_cls))}\")\n",
    "  predefined_sample_set_filename += f\"_{'-'.join(map(str, [train_portion, val_portion, test_portion]))}\"\n",
    "  if cv_type == 'group-shuffle' and num_train_types > 0:\n",
    "    predefined_sample_set_filename += f\"_{'-'.join(map(str, [num_train_types, num_val_types, num_test_types]))}\"\n",
    "  predefined_sample_set_filename += f'_{num_sets}sets'\n",
    "  filename_prefix = f\"{predefined_sample_set_filename}_{sel_set_idx}_{'-'.join(map(str, hidden_dim))}_{model_type}\"\n",
    "  filename = f'{filename_prefix}.ipynb'\n",
    "  nb = nbformat.read(f'{notebook_folder}/{template_file}', 4)\n",
    "  nb['cells'][0]['source'] = (\"import socket\\nif socket.gethostname() == 'dlm':\\n\"\n",
    "                              \"  %env CUDA_DEVICE_ORDER=PCI_BUS_ID\\n\"\n",
    "                              f\"  %env CUDA_VISIBLE_DEVICES={gpu_id}\")\n",
    "  nb['cells'][cell_idx]['source'] = '\\n'.join(\n",
    "    [f\"{k} = '{v}'\" if isinstance(v, str) else f'{k} = {v}' for k, v in args.items()])\n",
    "  if os.path.exists(f'{notebook_folder}/{filename}'):\n",
    "    print(f'To overwrite file {notebook_folder}/{filename}')\n",
    "  else:\n",
    "    print(f'To create file {notebook_folder}/{filename}')\n",
    "  if new_file:\n",
    "    nbformat.write(nb, f'{notebook_folder}/{filename}')\n",
    "  \n",
    "  if submit: # sometimes I just want to create files\n",
    "    if re.search('ccr.buffalo.edu$', socket.gethostname()):\n",
    "      command = f'sbatch {slurm_script} {notebook_folder}/{filename} {filename}'\n",
    "      subprocess.run(command, shell=True)\n",
    "      print(command)\n",
    "    else:\n",
    "      command = ['jupyter nbconvert', '--ExecutePreprocessor.timeout=360000',\n",
    "               '--ExecutePreprocessor.allow_errors=True', '--to notebook', '--execute']\n",
    "      command.append(f'{notebook_folder}/{filename} --output {filename}')\n",
    "      command = ' '.join(command)\n",
    "      start_time = time.time()\n",
    "      tmp = subprocess.run(command, shell=True)\n",
    "      end_time = time.time()\n",
    "      print(f'Time spent: {end_time-start_time:.2f}')\n",
    "  return filename_prefix\n",
    "\n",
    "def load_results(disease_type_str = '0', #0-1-6-8-10-11-16-17\n",
    "                  model_name = 'ml',\n",
    "                  sel_set_idx = 0,\n",
    "                  data_type_str = 'gene-mirna-rppa-methy',\n",
    "                  data_split_str = '70-10-20',\n",
    "                  hidden_dim_str = '100-100',\n",
    "                  filefolder = 'results',\n",
    "                  target_variable = 'pfi',\n",
    "                  return_variable='metric_all',\n",
    "                  filename=None, plot_acc=True, plot_loss=True):\n",
    "  if filename is None:\n",
    "    filename = (f'{filefolder}/{disease_type_str}_{data_type_str}_set{sel_set_idx}' \n",
    "                f'_{data_split_str}_{target_variable}_{hidden_dim_str}_{model_name}.pkl')\n",
    "    \n",
    "  with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "  if return_variable in data:\n",
    "    return np.array(data[return_variable])\n",
    "  metric = np.array(data['metric_all'])\n",
    "  confusion_mat = np.array(data['confusion_mat_all'])\n",
    "  model_names, split_names, metric_names = (data['model_names'], data['split_names'], \n",
    "                                            data['metric_names'])\n",
    "  # sanity check\n",
    "  assert metric.shape == (len(model_names), len(split_names), len(metric_names))\n",
    "  assert confusion_mat.shape[:2] == (len(model_names), len(split_names))\n",
    "  loss_his = data['loss_his_all']\n",
    "  acc_his = np.array(data['acc_his_all'])\n",
    "  title =  disease_type_str if len(disease_type_str)>2 else disease_stats[int(disease_type_str)]\n",
    "  if plot_acc and len(acc_his)>0:\n",
    "    for i, n in enumerate(split_names):\n",
    "      plot_history(acc_his[:, i].T, title=f'{title} {n} acc', \n",
    "                   indices=None, colors='rgbkmc', markers='ov+*,<',\n",
    "                       labels=model_names, linestyles=['']*6, markersize=3)\n",
    "    for i, n in enumerate(model_names):\n",
    "      plot_history(acc_his[i].T, title=f'{title} {n} acc', \n",
    "                   indices=None, colors='rgbkmc', markers='ov+*,<',\n",
    "                       labels=split_names, linestyles=['']*6, markersize=3)\n",
    "  if plot_loss and len(loss_his)>0:\n",
    "    for i, n in enumerate(model_names):\n",
    "      history = np.array(loss_his[i])\n",
    "      if history.ndim == 2:\n",
    "        plot_history(history.T, title=f'{title} {n} loss', indices=None, colors='rgbkmc', \n",
    "                     markers='ov+*,<',\n",
    "                       labels=split_names, linestyles=['']*6, markersize=3)\n",
    "      elif history.ndim == 3:\n",
    "        for j in range(history.shape[2]):\n",
    "           plot_history(history[:,:,j].T, title=f'{title} {n} loss{j}', indices=None, \n",
    "                        colors='rgbkmc', markers='ov+*,<',\n",
    "                       labels=split_names, linestyles=['']*6, markersize=3)\n",
    "      else:\n",
    "        raise ValueError(f'{filename} {n} loss has unexpected shape')\n",
    "  if return_variable == 'all':\n",
    "    return metric, confusion_mat, model_names, split_names, metric_names, acc_his, loss_his\n",
    "\n",
    "def new_notebook(kwargs, filename='notebook.ipynb', notebook_folder='.', \n",
    "               template_file='template.ipynb', \n",
    "               slurm_script='../gpu-slurm', \n",
    "               new_file=False, submit=False, run_local=True,\n",
    "               cell_idx=2, gpu_id=3):\n",
    "  \"\"\"Create notebook and run it on dlm or submit to ccr slurm\n",
    "  \"\"\"\n",
    "  nb = nbformat.read(f'{template_file}', 4)\n",
    "  nb['cells'][0]['source'] = (\"import socket\\nif socket.gethostname() == 'dlm':\\n\"\n",
    "                              \"  %env CUDA_DEVICE_ORDER=PCI_BUS_ID\\n\"\n",
    "                              f\"  %env CUDA_VISIBLE_DEVICES={gpu_id}\")\n",
    "  new_source = ''\n",
    "  for k, v in kwargs.items():\n",
    "    if isinstance(v, str):\n",
    "      new_source += f\"{k} = '{v}'\\n\"\n",
    "    elif isinstance(v, (list, tuple)):\n",
    "      new_source += '{} = [{}]\\n'.format(k, ', '.join(map(str, v)))\n",
    "    else:\n",
    "      new_source += f'{k} = {v}\\n'\n",
    "  new_source = new_source[:-1]   # remove the last '\\n'     \n",
    "  nb['cells'][cell_idx]['source'] = new_source\n",
    "  if os.path.exists(f'{notebook_folder}/{filename}'):\n",
    "    print(f'To overwrite file {notebook_folder}/{filename}')\n",
    "  else:\n",
    "    print(f'To create file {notebook_folder}/{filename}')\n",
    "  if new_file:\n",
    "    if not os.path.exists(notebook_folder):\n",
    "      os.makedirs(notebook_folder)\n",
    "    nbformat.write(nb, f'{notebook_folder}/{filename}')\n",
    "  \n",
    "  if submit: # sometimes I just want to create files\n",
    "    if re.search('ccr.buffalo.edu$', socket.gethostname()) and not run_local:\n",
    "      command = f'sbatch {slurm_script} {notebook_folder}/{filename} {filename}'\n",
    "      subprocess.run(command, shell=True)\n",
    "      print(command)\n",
    "    else:\n",
    "      command = ['jupyter nbconvert', '--ExecutePreprocessor.timeout=360000',\n",
    "               '--ExecutePreprocessor.allow_errors=True', '--to notebook', '--execute']\n",
    "      command.append(f'{notebook_folder}/{filename} --output {filename}')\n",
    "      command = ' '.join(command)\n",
    "      start_time = time.time()\n",
    "      tmp = subprocess.run(command, shell=True)\n",
    "      end_time = time.time()\n",
    "      print(f'Time spent: {end_time-start_time:.2f}')\n",
    "  return filename\n",
    "\n",
    "# data_folder = '/media/6T/.Trash-1014/pan-can-atlas/data/processed'\n",
    "# if not os.path.exists(data_folder):\n",
    "#   data_folder = ('/projects/academic/azhang/tianlema/deeplearning/'\n",
    "#     'pan-can-atlas/data/processed')\n",
    "# if not os.path.exists(data_folder):\n",
    "#   data_folder = 'F:/TCGA/Pan-Cancer-Atlas/data/processed'\n",
    "# with open(f'{data_folder}/sel_patient_clinical.pkl', 'rb') as f:\n",
    "#   data = pickle.load(f)\n",
    "#   disease_types = data['disease_types']\n",
    "#   disease_type_dict = data['disease_type_dict']\n",
    "#   pfi = data['pfi']\n",
    "# disease_stats = {}\n",
    "# for idx, name in disease_type_dict.items():\n",
    "#   cnt = list(collections.Counter(pfi[disease_types==idx]).values())\n",
    "#   if cnt[0] > 100 and cnt[1] > 100:\n",
    "#     disease_stats[idx] = f'{name}: {cnt}'\n",
    "#     print(name, idx, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jia_data = False\n",
    "randomize_labels = False\n",
    "init_num_gene = 5000\n",
    "min_num_gene_per_go = 5\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 100\n",
    "reduce_every = 100\n",
    "batch_size = None\n",
    "print_every = 1\n",
    "eval_every = 1\n",
    "return_best_val = True\n",
    "result_folder = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'overall_survival'\n",
    "seeds = range(10)\n",
    "if target_name == 'sample_type':\n",
    "  split_portion = [1, 1, 8]\n",
    "else:\n",
    "  split_portion = [6, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if target_name == 'sample_type':\n",
    "  # num['01'] >= 100 and num['11'] >= 50\n",
    "  cancer_type_done = ['LUAD', 'LIHC', 'BRCA', 'KIRC', 'PRAD', 'THCA']\n",
    "elif target_name == 'tumor_stage':\n",
    "  # min_num_per_cls = 100, min_num_per_type = 200\n",
    "  cancer_type_done = ['LUSC', 'LUAD', 'BLCA', 'BRCA', 'KIRC', 'THCA']\n",
    "elif target_name == 'pfi':\n",
    "  # num[0] >= 100, num[1] >= 100\n",
    "  cancer_type_done = ['BLCA', 'BRCA', 'COAD', 'HNSC', 'KIRC', 'LGG', 'LIHC', 'LUAD', \n",
    "                      'LUSC', 'OV', 'SARC', 'SKCM', 'STAD', 'UCEC']\n",
    "elif target_name == 'overall_survival':\n",
    "  # num[0] >= 100, num[1] >= 100\n",
    "  cancer_type_done = ['BLCA', 'BRCA', 'COAD', 'HNSC', 'KIRC', 'LGG', 'LIHC', 'LUAD',\n",
    "                      'LUSC', 'OV', 'SKCM', 'STAD']\n",
    "  \n",
    "for sel_proj_id in cancer_type_done:\n",
    "  for seed in seeds:\n",
    "    kwargs = {'use_jia_data': use_jia_data,\n",
    "          'sel_proj_id': sel_proj_id,\n",
    "          'target_name': target_name,\n",
    "          'split_portion': split_portion, \n",
    "         'seed': seed,\n",
    "         'randomize_labels': randomize_labels,\n",
    "         'init_num_gene': init_num_gene,\n",
    "         'min_num_gene_per_go': min_num_gene_per_go,\n",
    "         'lr': lr,\n",
    "         'weight_decay': weight_decay,\n",
    "         'num_epochs': num_epochs,\n",
    "         'reduce_every': reduce_every,\n",
    "         'batch_size': batch_size,\n",
    "         'print_every': print_every,\n",
    "         'eval_every': eval_every,\n",
    "         'return_best_val': return_best_val,\n",
    "             'result_folder': result_folder}\n",
    "    if use_jia_data:\n",
    "      sel_proj_id = 'JIA'\n",
    "    split_portion_str = np.array(split_portion)\n",
    "    split_portion_str = split_portion_str * 100 / split_portion_str.sum()\n",
    "    split_portion_str = '-'.join(map(lambda s: str(int(s)), split_portion_str))\n",
    "    res_filename_prefix = f'{sel_proj_id}_{split_portion_str}_seed{seed}_{target_name}'\n",
    "    filename = f'{res_filename_prefix}.ipynb'\n",
    "    new_notebook(kwargs, filename=filename, notebook_folder='results', \n",
    "             template_file='GeneNet.ipynb', \n",
    "             slurm_script='gpu-slurm', \n",
    "             new_file=True, submit=True, run_local=True,\n",
    "             cell_idx=2, gpu_id=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
