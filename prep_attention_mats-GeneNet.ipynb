{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "if socket.gethostname() == 'dlm':\n",
    "  %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "  %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "import requests, zipfile, io\n",
    "import pickle\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "import networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lib_path = 'I:/code'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/media/6T/.tianle/.lib'\n",
    "if not os.path.exists(lib_path):\n",
    "  lib_path = '/projects/academic/azhang/tianlema/lib'\n",
    "if os.path.exists(lib_path) and lib_path not in sys.path:\n",
    "  sys.path.append(lib_path)\n",
    "\n",
    "from dl.models.transformer import MultiheadAttention, EncoderAttention, DecoderAttention, Transformer, StackedEncoder\n",
    "from dl.models.dag import *\n",
    "from dl.models.basic_models import *\n",
    "from dl.models.factor_graph import BipartiteGraph1d, BipartiteGraph, EmbedCell, GeneNet\n",
    "from dl.utils.visualization.visualization import *\n",
    "from dl.utils.outlier import *\n",
    "from dl.utils.train import *\n",
    "from autoencoder.autoencoder import *\n",
    "from vin.vin import *\n",
    "from dl.utils.utils import get_overlap_samples, filter_clinical_dict, get_target_variable\n",
    "from dl.utils.utils import get_shuffled_data, target_to_numpy, discrete_to_id, get_mi_acc\n",
    "from dl.utils.utils import get_label_distribution, normalize_continuous_variable, tensor_to_num\n",
    "from dl.utils.utils import adj_list_to_mat, adj_list_to_attention_mats, count_model_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "  print('Using GPU:)')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Using CPU:(')\n",
    "  \n",
    "inf = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "save_filepath = 'htseq_cnt_go_clinical.pkl'\n",
    "if overwrite or not os.path.exists(save_filepath):\n",
    "  print(f'Write file {save_filepath}')\n",
    "  with open(save_filepath, 'wb') as f:\n",
    "      pickle.dump({'htseq_cnt_mat': htseq_cnt_mat, 'gene_ids': gene_ids, \n",
    "                   'aliquot_ids': aliquot_ids, 'go_gene_list': go_gene_list, \n",
    "                  'go_edges': go_edges, 'string_ppi': ppi, \n",
    "                  'percent_tumor_nuclei': percent_tumor_nuclei,\n",
    "                  'sample_type': sample_type, 'diagnoses': diagnoses,\n",
    "                  'clinical': clinical, 'cases': cases, 'survival_plot': survival_plot}, \n",
    "                 f)\n",
    "\n",
    "with open(save_filepath, 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "go_gene_list = data['go_gene_list']\n",
    "go_edges = data['go_edges']\n",
    "gene_ids = data['gene_ids']\n",
    "htseq_cnt_mat = data['htseq_cnt_mat']\n",
    "ppi = data['string_ppi']\n",
    "\n",
    "mat = np.log2(htseq_cnt_mat+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 11417) (5000,)\n"
     ]
    }
   ],
   "source": [
    "num_gene = 5000\n",
    "min_num_gene_per_go = 5\n",
    "\n",
    "mean = mat.mean(axis=1)\n",
    "std = mat.std(axis=1)\n",
    "idx = np.argsort(-std)[:num_gene]\n",
    "gene_ids = gene_ids[idx]\n",
    "mat = mat[idx]\n",
    "print(mat.shape, gene_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4994,) (4994, 11417) 5000\n",
      "(4994,) (4994, 11417) 4994\n"
     ]
    }
   ],
   "source": [
    "common_geneset = set(functools.reduce(lambda x,y: x+y, go_gene_list.values()))\n",
    "while common_geneset != set(gene_ids):\n",
    "  common_geneset = common_geneset.intersection(gene_ids)\n",
    "  # remove genes that are not in common_geneset\n",
    "  go_gene_list = {k: sorted(common_geneset.intersection(v)) for k, v in go_gene_list.items()}\n",
    "  # remove GO terms that have less than min_num_gene_per_go genes\n",
    "  go_gene_list = {k: v for k, v in go_gene_list.items() if len(v)>=min_num_gene_per_go}\n",
    "  go_ids = set(go_gene_list)\n",
    "  # some genes may no longer appear in go_gene_list, remove them\n",
    "  common_geneset_new = set(functools.reduce(lambda x,y: x+y, go_gene_list.values()))\n",
    "  idx_gene = np.array([(i, g) for i, g in enumerate(gene_ids) if g in common_geneset_new])\n",
    "  gene_ids = idx_gene[:, 1]\n",
    "  mat = mat[idx_gene[:, 0].astype('int')]\n",
    "  print(gene_ids.shape, mat.shape, len(common_geneset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1194), (1, 467), (2, 214), (3, 100), (4, 59), (5, 28), (6, 10), (7, 4), (8, 3), (9, 372)]\n",
      "2696 4994 4\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4 # len(chain_graph_go)\n",
    "name_to_id_gene = {n:i for i, n in enumerate(gene_ids)}\n",
    "num_gene = len(gene_ids)\n",
    "\n",
    "num_go = len(go_gene_list)\n",
    "go_ids = set(go_gene_list)\n",
    "go_edges = go_edges[[s[0] in go_ids and s[1] in go_ids for s in go_edges]]\n",
    "name_to_id_go, chain_graph_go = get_topological_order(go_edges[:,[1,0]])\n",
    "print([(i, len(v)) for i, v in enumerate(chain_graph_go)])\n",
    "# there are some isolated GO terms; include them as well in the model\n",
    "for go in go_gene_list:\n",
    "  if go not in name_to_id_go:\n",
    "    name_to_id_go[go] = len(name_to_id_go)\n",
    "# prepare dag\n",
    "dag = collections.defaultdict(list)\n",
    "for s in go_edges:\n",
    "  left = name_to_id_go[s[0]]\n",
    "  right = name_to_id_go[s[1]]\n",
    "  dag[right].append(left)\n",
    "dag = {k: sorted(set(v)) for k, v in dag.items()}\n",
    "# prepare bigraph\n",
    "id_to_name_go = {i: n for n, i in name_to_id_go.items()}\n",
    "num_leaf_go = min(dag)\n",
    "bigraph = []\n",
    "for i in range(num_leaf_go):\n",
    "  bigraph.append(sorted([name_to_id_gene[v] for v in go_gene_list[id_to_name_go[i]]]))\n",
    "print(num_go, num_gene, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 4994 nodes and 147228 edges\n",
      "Warning: there are 617 isolated node(s)!\n",
      "Graph with 2696 nodes and 2815 edges\n",
      "Bipartite graph: in_features=4994, out_features=2696\n",
      "Time spent on generating attention_mats 3.1285510063171387 s\n"
     ]
    }
   ],
   "source": [
    "attention_mats_filepath = f'attention_mats-{num_gene}-{num_go}-{num_steps}-GeneNet.pkl'\n",
    "overwrite = True\n",
    "if overwrite or not os.path.exists(attention_mats_filepath):\n",
    "#   gene_gene_adj_mat, _ = adj_list_to_mat(ppi, name_to_id=name_to_id_gene, bipartite=False, \n",
    "#                                        add_self_loop=True, symmetric=True, return_list=False)\n",
    "  gene_gene_adj_mat = np.zeros((num_gene, num_gene))\n",
    "  for s in ppi:\n",
    "    if s[0] in name_to_id_gene and s[1] in name_to_id_gene:\n",
    "      left = name_to_id_gene[s[0]]\n",
    "      right = name_to_id_gene[s[1]]\n",
    "      gene_gene_adj_mat[left, right] = float(s[2])/1000\n",
    "  gene_gene_adj_mat[range(num_gene), range(num_gene)] = 1\n",
    "  \n",
    "  go_go_adj_mat, _ = adj_list_to_mat(go_edges, name_to_id=name_to_id_go, bipartite=False, \n",
    "                   add_self_loop=False, symmetric=False, return_list=False)\n",
    "  \n",
    "  gene_go_adj_mat = np.zeros((num_gene, num_go))\n",
    "  for k, v in go_gene_list.items():\n",
    "    go_id = name_to_id_go[k]\n",
    "    for g in v:\n",
    "      gene_id = name_to_id_gene[g]\n",
    "      gene_go_adj_mat[gene_id, go_id] = 1\n",
    "      \n",
    "  attention_mats = {}\n",
    "  start_time = time.time()\n",
    "  attention_mats['gene1->gene0'], id_to_name_gene = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps, name_to_id=name_to_id_gene, bipartite=False, \n",
    "    add_self_loop=True, symmetric=True, target_to_source=None, use_transition_matrix=True, \n",
    "    Ms=gene_gene_adj_mat, softmax_normalization=False, min_value=-100, device=device)\n",
    "\n",
    "  attention_mats['pathway1->pathway0'], id_to_name_go = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps, name_to_id=name_to_id_go, bipartite=False, \n",
    "    add_self_loop=False, symmetric=False, target_to_source=None, use_transition_matrix=True, \n",
    "    Ms=go_go_adj_mat, softmax_normalization=False, min_value=-100, device=device)\n",
    "\n",
    "  mats, _ = adj_list_to_attention_mats(\n",
    "    adj_list=None, num_steps=num_steps*2, name_to_id=[name_to_id_gene, name_to_id_go], \n",
    "    bipartite=True, add_self_loop=False, symmetric=False, target_to_source=None, \n",
    "    use_transition_matrix=True, Ms=gene_go_adj_mat, softmax_normalization=False, min_value=-100, \n",
    "    device=device)\n",
    "  # this is very tricky: \n",
    "  # the even positions are all gene->pathway in mats[0], while odd ones gene->gene\n",
    "  attention_mats['gene0->pathway1'] = [m for i, m in enumerate(mats[0]) if i%2==0]\n",
    "  attention_mats['pathway0->gene1'] = [m for i, m in enumerate(mats[1]) if i%2==0]\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(f'Time spent on generating attention_mats {end_time - start_time} s')\n",
    "\n",
    "#   start_time = time.time()\n",
    "#   with open(attention_mats_filepath, 'wb') as f:\n",
    "#     if device == torch.device('cuda'):\n",
    "#       for k, v in attention_mats.items():\n",
    "#         for i in range(len(v)):\n",
    "#           v[i] = v[i].detach().cpu().numpy()\n",
    "#     pickle.dump(attention_mats, f)\n",
    "#   end_time = time.time()\n",
    "#   print(f'Time spent on writing attention_mats {end_time - start_time} s')\n",
    "\n",
    "# start_time = time.time()\n",
    "# with open('attention_mats-GeneNet.pkl', 'rb') as f:\n",
    "#   attention_mats = pickle.load(f)\n",
    "#   for k, v in attention_mats.items():\n",
    "#     for i in range(len(v)):\n",
    "#       v[i] = torch.tensor(v[i]).float().to(device)\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on loading attention_mats {end_time - start_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_dag_layer = False\n",
    "# dag_in_channel_list = [1,1,1]\n",
    "# dag_kwargs = {'residual':True, 'duplicate_dag':True}\n",
    "# batch_size = 500\n",
    "# model = GeneNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "#                 use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "#                 dag_kwargs=dag_kwargs,\n",
    "#                 nonlinearity=nn.ReLU(), use_layer_norm=True).to(device)\n",
    "\n",
    "# x = torch.randn(batch_size, num_gene).to(device)\n",
    "\n",
    "# start_time = time.time()\n",
    "# y = model(x, attention_mats=attention_mats, max_num_layers=num_steps, min_num_layers=num_steps, \n",
    "#           return_layers='all')\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "# print(y[0].shape, y[1].shape, y[2].shape, y[3].shape)\n",
    "\n",
    "# start_time = time.time()\n",
    "# loss = y[0].sum() + y[1].sum() + y[2].sum() + y[3].sum()\n",
    "# loss.backward()\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on backward pass {end_time - start_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_dag_layer = True\n",
    "# dag_in_channel_list = [1]\n",
    "# dag_kwargs = {'residual':True, 'duplicate_dag':True}\n",
    "# batch_size = 500\n",
    "# model = GeneNet(num_genes=num_gene, num_pathways=num_go, attention_mats=None, dense=True,\n",
    "#                 use_dag_layer=use_dag_layer, dag=dag, dag_in_channel_list=dag_in_channel_list, \n",
    "#                 dag_kwargs=dag_kwargs,\n",
    "#                 nonlinearity=nn.ReLU(), use_layer_norm=True).to(device)\n",
    "\n",
    "# x = torch.randn(batch_size, num_gene).to(device)\n",
    "\n",
    "# start_time = time.time()\n",
    "# y = model(x, attention_mats=attention_mats, max_num_layers=num_steps, min_num_layers=num_steps, \n",
    "#           return_layers='all')\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "# print(y[0].shape, y[1].shape, y[2].shape, y[3].shape)\n",
    "\n",
    "# start_time = time.time()\n",
    "# loss = y[0].sum() + y[1].sum() + y[2].sum() + y[3].sum()\n",
    "# loss.backward()\n",
    "# end_time = time.time()\n",
    "# print(f'Time spent on backward pass {end_time - start_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2451\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 5\n",
    "in_channels_list = [5]\n",
    "key_dim = 5\n",
    "value_dim = 5\n",
    "fc_dim = 5\n",
    "dim_per_cls = 1\n",
    "num_heads = 1\n",
    "num_attention = 1\n",
    "num_go = max(dag)+1\n",
    "print(num_go)\n",
    "graph_encoder = [m[:num_go][:,:num_go] for m in attention_mats['pathway1->pathway0']]\n",
    "graph_weight_encoder = 0.5\n",
    "knn = 50\n",
    "\n",
    "residual = True \n",
    "duplicate_dag = True \n",
    "gibbs_sampling = True \n",
    "duplicated_attention = True \n",
    "feature_max_norm = 1\n",
    "use_layer_norm = True \n",
    "bias = True \n",
    "nonlinearity = nn.ReLU()\n",
    "\n",
    "batch_size = 15\n",
    "x = torch.randn(batch_size, num_gene).to(device)\n",
    "num_features = x.size(1)\n",
    "num_cls = 2\n",
    "\n",
    "model = DAGEncoder(num_features=num_features, embedding_dim=embedding_dim, \n",
    "         in_channels_list=in_channels_list, \n",
    "         bigraph=bigraph, dag=dag, key_dim=key_dim, value_dim=value_dim, fc_dim=fc_dim, \n",
    "         num_cls=num_cls, dim_per_cls=dim_per_cls, feature_max_norm=feature_max_norm, \n",
    "         use_layer_norm=use_layer_norm, bias=bias, \n",
    "         nonlinearity=nonlinearity, residual=residual, duplicate_dag=duplicate_dag, \n",
    "         gibbs_sampling=gibbs_sampling, num_heads=num_heads, num_attention=num_attention,\n",
    "         knn=knn, duplicated_attention=duplicated_attention,\n",
    "         graph_encoder=None, graph_weight_encoder=graph_weight_encoder, \n",
    "         graph_decoder=None, graph_weight_decoder=0.5, use_encoders=True).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "y = model(x, graph_encoder=graph_encoder)\n",
    "end_time = time.time()\n",
    "print(f'Time spent on forward pass {end_time - start_time} s')\n",
    "\n",
    "start_time = time.time()\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "end_time = time.time()\n",
    "print(f'Time spent on backward pass {end_time - start_time} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
