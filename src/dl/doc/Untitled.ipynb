{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence (seq2seq) model\n",
    "This is based on paper \"Attention is all you need\".\n",
    "\n",
    "Applications include machine translation.\n",
    "\n",
    "Seq2seq models follows the basic architecture of autoencoder, which uses multi-head attention mechanism.\n",
    "\n",
    "In the following, I will implement seq2seq model based on my own understanding step by step. This can be different from the model described in the seq2seq model paper and tensorflow seq2seq package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(x, n, forced=False):\n",
    "  \"\"\"Expand x to a list of n\n",
    "  If x is already a list of length n and force is False, then do nothing. \n",
    "  Otherwise, return [x]*n\n",
    "  \"\"\"\n",
    "  if forced or not isinstance(x, collections.Iterable):\n",
    "    return [x]*n\n",
    "  if len(x) != n:\n",
    "    return [x]*n\n",
    "  else:\n",
    "    return x\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "  r\"\"\"Multi-layer perceptron\n",
    "  Args:\n",
    "    in_dim: int, input feature dimension\n",
    "    hidden_dims: sequence of int, hidden dimensions; hidden_dims[-1] is the output dimension\n",
    "    nonlinearity: default nn.ReLU(); can be changed to other nonlinearities\n",
    "    dense: if True, use DenseNet architecture, \n",
    "      i.e., concatenate all previous output as current input\n",
    "    residual: if True, use ResNet architecture. That's to say:\n",
    "      add the weighted average (decided by weighted_avg function) of previous outputs (after activations) to current affine output, \n",
    "      and then pass it to nonlinearity\n",
    "    last_nonlinearity: default False; if True, add nonlinearity to output layer.\n",
    "    forward_input: if True, forward input layer to subsequential layers when either dense or residual is True\n",
    "    return_all: If True, return a list of output; otherwise, return the output from the last layer\n",
    "    residual_mode: default 'last', only add the previous (nonlinear) output to current affine output;\n",
    "      only used when residual is True; If 'weighted', then calculate a default weighted average of all previous outputs\n",
    "      and add it to current affine output before passing it to nonlinearity. \n",
    "      To do: make the weight learnable\n",
    "  \n",
    "  Shape:\n",
    "    Input: (N, *, in_dim)\n",
    "    Output: if return_all is False, then (N, *, out_dim); else return a list of tensors \n",
    "    (depend on forward_input, dense, and residual)\n",
    "  \n",
    "  Attributes:\n",
    "    A list of weights and biases from nn.Linear modules; the dimensions depend on in_dims, hidden_dims,\n",
    "      dense, residual, forward_input\n",
    "  \n",
    "  Examples:\n",
    "  \n",
    "    >>> x = torch.randn(3,4,5)\n",
    "    >>> model = MultiLayerPerceptron(5, [5,5,5], dense=True)\n",
    "    >>> model(x).shape\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, in_dim, hidden_dims, nonlinearity=nn.ReLU(inplace=True), bias=True, \n",
    "               dense=True, residual=False, last_nonlinearity=False, forward_input=False, return_all=False, \n",
    "               residual_mode='last'):\n",
    "    super(MultiLayerPerceptron, self).__init__()\n",
    "    num_layers = len(hidden_dims)\n",
    "    self.dense = dense\n",
    "    self.residual = residual\n",
    "    self.last_nonlinearity = last_nonlinearity\n",
    "    self.forward_input = forward_input\n",
    "    self.return_all = return_all\n",
    "    self.residual_mode = residual_mode\n",
    "    # make sure the dimensions are right\n",
    "    assert not (dense and residual)\n",
    "    if residual:\n",
    "      for i in range(1, num_layers):\n",
    "        assert hidden_dims[i]==hidden_dims[i-1]\n",
    "      if forward_input:\n",
    "        assert in_dim==hidden_dims[0]\n",
    "    \n",
    "    # nonlinearity and bias can be a set layer by layer by providing a list input\n",
    "    nonlinearities = get_list(nonlinearity, num_layers if last_nonlinearity else num_layers-1)\n",
    "    biases = get_list(bias, num_layers)\n",
    "    \n",
    "    self.layers = nn.Sequential()\n",
    "    for i in range(num_layers):\n",
    "      out_dim = hidden_dims[i]\n",
    "      self.layers.add_module('linear{}'.format(i), nn.Linear(in_dim, out_dim, bias=biases[i]))\n",
    "      if i < num_layers-1 or last_nonlinearity:\n",
    "        self.layers.add_module('activation{}'.format(i), nonlinearities[i])\n",
    "      # prepare for input dimension for next layer\n",
    "      if dense:\n",
    "        if i==0 and not forward_input:\n",
    "          in_dim = 0\n",
    "        in_dim += hidden_dims[i]\n",
    "      else:\n",
    "        in_dim = hidden_dims[i]\n",
    "  \n",
    "  def weighted_avg(self, y, mode='last', weight=None):\n",
    "    if mode == 'last':\n",
    "      return y[-1]\n",
    "    if model == 'unweighted':\n",
    "      return torch.cat(y, dim=-1).mean(-1)\n",
    "    if mode == 'weighted':\n",
    "      if weight is None:\n",
    "        weight = torch.tensor([i for i in range(1, len(y)+1)], device=device)\n",
    "      weight = weight / weight.sum()\n",
    "      return (torch.cat(y, dim=-1) * weight).sum(-1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if self.forward_input:\n",
    "      y = [x]\n",
    "    else:\n",
    "      y = []\n",
    "    out = x\n",
    "    for n, m in self.layers._modules.items():\n",
    "      out = m(out)\n",
    "      if n.startswith('activation'):\n",
    "        y.append(out)\n",
    "        if self.dense:\n",
    "          out = torch.cat(y, dim=-1)\n",
    "      if n.startswith('linear') and self.residual and len(y)>0:\n",
    "        out = out + self.weighted_avg(y, mode=self.residual_mode)\n",
    "    \n",
    "    if self.return_all:\n",
    "      if not self.last_nonlinearity:\n",
    "        y.append(out)\n",
    "      return y\n",
    "    else:\n",
    "      return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  r\"\"\"Use multi-head self attention mechanism to learn sequence embedding.\n",
    "  Args:\n",
    "    in_dim: int; input feature dimension\n",
    "    out_dim: int; output feature dimension\n",
    "    key_dim: int or a list of num_heads int; \n",
    "      map input to (key, value) and use keys to calculate weights (attention) \n",
    "    value_dim: int or a list of num_heads int; if None, set it to be out_dim\n",
    "    num_heads: int\n",
    "    mask: if True, each element in a sequence only attends to itself and its left side; \n",
    "      useful for decoder\n",
    "    knn: int; only attend to the top k elements with the highest unnormalized attention\n",
    "    \n",
    "  Shape:\n",
    "    Input: (N, seq_len, in_dim) for most cases; (N, *, seq, in_dim) is also possible\n",
    "    Output: change the last dimension of input to out_dim\n",
    "  \n",
    "  Attributes:\n",
    "    In the end all parameters are from nn.Linear modules. \n",
    "    keys and values are two nn.ModuleList instances with num_heads of two-layer perceptron\n",
    "    In the final layer we concatenate feature vector from all heads and pass it to a two-layer\n",
    "    perceptron and get the final output\n",
    "  \n",
    "  Examples:\n",
    "  \n",
    "    >>> x = torch.randn(3,5,7)\n",
    "    >>> model = MultiHeadAttention(7,11,13,17,19, mask=False, knn=1)\n",
    "    >>> model(x).shape\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, in_dim, out_dim, key_dim, value_dim=None, \n",
    "               num_heads=1, mask=False, knn=None):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    if value_dim is None:\n",
    "      value_dim = out_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.mask = mask\n",
    "    self.knn = knn\n",
    "    key_dims = get_list(key_dim, num_heads)\n",
    "    value_dims = get_list(value_dim, num_heads)\n",
    "    self.keys = nn.ModuleList([MultiLayerPerceptron(in_dim, [key_dims[i]]*2) \n",
    "                               for i in range(num_heads)])\n",
    "    self.values = nn.ModuleList([MultiLayerPerceptron(in_dim, [value_dims[i]]*2) \n",
    "                                 for i in range(num_heads)])\n",
    "    self.out = MultiLayerPerceptron(sum(value_dims), [out_dim]*2)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    y = []\n",
    "    for i in range(self.num_heads):\n",
    "      keys = self.keys[i](x)\n",
    "      values = self.values[i](x)\n",
    "      # inner product as unnormalized attention\n",
    "      att = (keys.unsqueeze(-2) * keys.unsqueeze(-3)).sum(-1) \n",
    "      if self.mask:\n",
    "        # mask the upper triangle to be float('-Inf')\n",
    "        tmp = att.new_tensor(range(att.size(-1))).expand_as(att)\n",
    "        idx = torch.nonzero(tmp > tmp.transpose(-1,-2))\n",
    "        idx = [idx[:,i] for i in range(idx.size(1))]\n",
    "        att[idx] = float('-Inf')\n",
    "      if self.knn and self.knn < att.size(-1):\n",
    "        # tricky: put Non-topk values to '-Inf'\n",
    "        att.scatter_(-1, att.topk(att.size(-1) - self.knn, -1, largest=False)[1], float('-Inf'))\n",
    "      # Use softmax to normalize attention; # To do: alternative to softmax\n",
    "      att = torch.nn.functional.softmax(att, dim=-1)\n",
    "      # tricky: \n",
    "      y.append((values.unsqueeze(-3) * att.unsqueeze(-1)).sum(-2))\n",
    "      \n",
    "    return self.out(torch.cat(y, dim=-1))\n",
    "  \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  \"\"\"Stacked MultiHeadAttention layers\n",
    "  \"\"\"\n",
    "  def __init__(self, num_layers, in_dim, out_dim, key_dim, value_dim=None, num_heads=1, \n",
    "               mask=False, knn=None, residual=True, normalization='layer_norm', return_all=False):\n",
    "    super(Encoder, self).__init__()\n",
    "    # We can set out_dim layer by layer, similar to key_dim, value_dim, num_heads\n",
    "    out_dim = [in_dim] + get_list(out_dim, num_layers)\n",
    "    key_dim = get_list(key_dim, num_layers)\n",
    "    value_dim = get_list(value_dim, num_layers)\n",
    "    num_heads = get_list(num_heads, num_layers)\n",
    "    self.num_layers = num_layers\n",
    "    self.residual = residual\n",
    "    self.return_all = return_all\n",
    "    self.normalization = normalization\n",
    "    if residual:\n",
    "      for i in range(num_layers):\n",
    "        assert out_dim[i] == out_dim[i+1]\n",
    "    self.attentions = nn.ModuleList([MultiHeadAttention(\n",
    "      out_dim[i], out_dim[i+1], key_dim[i], value_dim[i], num_heads[i], mask, knn) \n",
    "                                     for i in range(num_layers)])\n",
    "    self.perceptrons = nn.ModuleList([MultiLayerPerceptron(out_dim[i+1], [out_dim[i+1]]*2) \n",
    "                                     for i in range(num_layers)])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    y = []\n",
    "    out = x\n",
    "    for i in range(self.num_layers):\n",
    "      if self.residual:\n",
    "        out = self.attentions[i](out) + out\n",
    "      else:\n",
    "        out = self.attentions[i](out)\n",
    "      if self.normalization == 'layer_norm':\n",
    "        out = nn.functional.layer_norm(out, (out.size(-1),))\n",
    "      # perceptron\n",
    "      if self.residual:\n",
    "        out = self.perceptrons[i](out) + out\n",
    "      else:\n",
    "        out = self.perceptrons[i](out)\n",
    "      if self.normalization == 'layer_norm':\n",
    "        out = nn.functional.layer_norm(out, (out.size(-1),))\n",
    "      y.append(out)\n",
    "    if self.return_all:\n",
    "      return y\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 7])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,5,7)\n",
    "\n",
    "model = Encoder(1, 7, 7, 11, mask=True, knn=3)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,5,5)\n",
    "idx = torch.nonzero(x > x.transpose(-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2]),\n",
       " tensor([0, 1, 1, 2, 2, 2, 3, 4, 4, 4, 0, 0, 0, 1, 1, 2, 3, 3, 4, 4, 0, 0, 0, 1,\n",
       "         1, 2, 2, 3, 3, 4]),\n",
       " tensor([3, 0, 3, 0, 1, 3, 4, 0, 1, 2, 1, 2, 3, 2, 3, 4, 2, 4, 0, 1, 1, 2, 4, 3,\n",
       "         4, 1, 3, 0, 4, 2])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx[:, i] for i in range(idx.size(1))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
